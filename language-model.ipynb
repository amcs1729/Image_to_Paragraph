{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install -q pyyaml h5py\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\nfrom keras.utils import np_utils\nimport tensorflow as tf\nimport gc\nimport string\nfrom tqdm import tqdm\nimport os\nimport math","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nglove_dir = '../input/glove/glove.6B.200d.txt'\nembeddings_index = {} # empty dictionary\nf = open(glove_dir, encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef clean_captions(caption_list):\n        table = str.maketrans('', '', string.punctuation)\n \n        for i in range(len(caption_list)):\n                desc = caption_list[i]\n                # tokenize\n                desc = desc.split()\n                # convert to lower case\n                desc = [word.lower() for word in desc]\n                # remove punctuation from each token\n                desc = [w.translate(table) for w in desc]\n                # remove hanging 's' and 'a'\n                desc = [word for word in desc if len(word)>1]\n                # remove tokens with numbers in them\n                desc = [word for word in desc if word.isalpha()]\n                # store as string\n                caption_list[i] =  ' '.join(desc)\n \n        return (caption_list)\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_filename = '../input/text-data/1342-0 (1).txt'\ndoc = load_doc(in_filename)\nlines = doc.split('\\n')\nlines = clean_captions(lines)\nlines = lines[0: round((len(lines)/5))]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines_new= []\nfor i in tqdm(range(len(lines))):\n  if (lines[i] != ''):\n    lines_new.append(lines[i])\nlines = lines_new\ndel lines_new\ngc.collect()","execution_count":10,"outputs":[{"output_type":"stream","text":"100%|██████████| 25395/25395 [00:00<00:00, 906165.78it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"78"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = list()\nfor i, key  in enumerate(lines):\n    word_list = (lines[i].split(' '))\n    for word in word_list:\n \n      words.append(word)\nvocabulary = set(words)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nixtoword = {}\nwordtoix = {}\nix = 1\nfor w in list(vocabulary):\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nixtoword[0] = '<unk>'\nwordtoix['<unk>'] = 0","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def same_length_caption(caption , max_len=50):\n \n    '''\n    Takes caption as input and makes them of equal length\n    \n    Parameters:-\n    caption (list) - The list of embedded caption to be made of particular length\n    max_len (int) - The max length of the caption\n    \n    Return type:-\n    \n    caption (list) :- Returns a list with zero padding of length = max_len\n    '''\n    \n    \n    if(len(caption) == max_len):\n        return (caption)\n    else:\n        for i in range((max_len-len(caption))):\n            caption.append(0)\n    return caption\ndef word_to_ix(caption , vocab):\n    '''\n    Maps the words to integers according to custom vocabulary\n    \n    Parameters:-\n    caption (list) - The caption to be embedded\n    vocab (dict) - The custom mapping that wil be used as vocabulary\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after mapping them according to 'vocab'\n    '''\n        \n    transformed_caption=[]\n    for word in caption:\n        if (word in wordtoix.keys()):\n            transformed_caption.append(wordtoix[word])\n        else:\n            transformed_caption.append(wordtoix['<unk>'])\n    return (transformed_caption)\n        \ndef ix_to_word(caption , vocab):\n    '''\n    Takes caption as input and maps them to words as defined by 'vocab'\n    \n    Parameters:-\n    caption (list) - The list of embedded caption to be made of particular length\n    vocab (dict) - The dictionary that wil be used as mapping\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after converting respective integers to words according to vocab\n    '''\n    \n    transformed_caption=[]\n    for word in caption:\n        transformed_caption.append(ixtoword[word])\n\n    return (transformed_caption)\n","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n\n#words = list()\n#for i, key  in enumerate(lines):\n#    word_list = (lines[i].split(' '))\n#    for word in word_list:\n \n#      words.append(word)\n#vocabulary = set(words)\n\n#from collections import Counter \n  \n#def removeElements(lst, k): \n#    counted = Counter(lst) \n#    return [el for el in lst if counted[el] >= k] \n \n#k = 8\n#vocabulary_new = ((removeElements(flat_list, k))) \n#vocabulary_new = set(vocabulary_new)\n#vocabulary.update(['<unk>'])\n\n\ntrain_len = 12+1\ntext_sequences = []\nfor i in range(train_len,len(words)):\n    seq = words[i-train_len:i]\n    text_sequences.append(seq)\n\n\ndef generator(batch_size=32):\n    \"\"\"\n    Yields the next training batch.\n    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].\n    \"\"\"\n    num_samples = len(text_sequences)\n    \n    while True:\n        for offset in range(0, num_samples, batch_size):\n            batch_samples = text_sequences[offset:offset+batch_size]\n \n            input_seq=[]\n            output_seq=[]\n \n            for batch_sample in batch_samples:\n \n \n                #caption_text = batch_samples.at[batch_sample , 'image_caption']\n                #caption = caption_text.split()\n                batch_sample = word_to_ix(batch_sample , wordtoix)\n                #caption = same_length_caption(caption , max_len = 34)\n \n                #print(type(batch_sample))\n                #print(len(batch_sample))\n                \n                \n                #samples = word_to_ix(samples , wordtoix)\n \n                input_seq.append(np.array(batch_sample[0:-1]))\n                output_seq.append(np.array(batch_sample[-1:]))\n \n                #print(input_seq)\n                #print(output_seq)\n            \n            input_seq = np.array(input_seq)\n            output_seq = np.array(output_seq)\n \n            yield(input_seq , output_seq)\n  \nvocab_size = len(vocabulary) + 1\nembedding_dim = 200\n# Get 200-dim dense vector for each of the 10000 words in out vocabulary\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    #if i < max_words:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in the embedding index will be all zeros\n        embedding_matrix[i] = embedding_vector","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n\n  inputs1 = tf.keras.layers.Input(shape=(12))\n  se1 = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim)(inputs1)\n  se2 = tf.keras.layers.GRU(512,return_sequences=True )(se1)\n  se3 = tf.keras.layers.GRU(512,return_sequences=False )(se2)\n\n  output = tf.keras.layers.Dense(vocab_size,activation='softmax')(se3)\n\n  model = tf.keras.Model(inputs=[inputs1],outputs=[output])\n  opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n  model.compile(loss= 'sparse_categorical_crossentropy', optimizer=opt)\n\n  return model\n\n\n\nmodel = build_model()\n\n#opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n#model.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt , metrics= ['accuracy'])\n\nmodel.layers[1].set_weights([embedding_matrix])\nmodel.layers[1].trainable = False\n\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_red = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.2, patience=3, verbose=1, mode='auto',\n    min_delta=0.0001, cooldown=0, min_lr=0.0000001)\ncallbacks = [lr_red ]\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 2048","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = generator(batch_size= BATCH_SIZE)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.backend import manual_variable_initialization\nmanual_variable_initialization(True)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=(len(text_sequences)/BATCH_SIZE),\n        epochs=200, \n        verbose=1,\n        callbacks = callbacks\n        )\n","execution_count":41,"outputs":[{"output_type":"stream","text":"Epoch 1/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.2099 - lr: 8.0000e-05\nEpoch 2/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.2079 - lr: 8.0000e-05\nEpoch 3/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.2063 - lr: 8.0000e-05\nEpoch 4/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.2044 - lr: 8.0000e-05\nEpoch 5/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.2024 - lr: 8.0000e-05\nEpoch 6/200\n106/105 [==============================] - 12s 114ms/step - loss: 0.2008 - lr: 8.0000e-05\nEpoch 7/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1993 - lr: 8.0000e-05\nEpoch 8/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1974 - lr: 8.0000e-05\nEpoch 9/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1956 - lr: 8.0000e-05\nEpoch 10/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1944 - lr: 8.0000e-05\nEpoch 11/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1925 - lr: 8.0000e-05\nEpoch 12/200\n106/105 [==============================] - 12s 116ms/step - loss: 0.1907 - lr: 8.0000e-05\nEpoch 13/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1900 - lr: 8.0000e-05\nEpoch 14/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1879 - lr: 8.0000e-05\nEpoch 15/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1864 - lr: 8.0000e-05\nEpoch 16/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1846 - lr: 8.0000e-05\nEpoch 17/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1830 - lr: 8.0000e-05\nEpoch 18/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1811 - lr: 8.0000e-05\nEpoch 19/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1795 - lr: 8.0000e-05\nEpoch 20/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1782 - lr: 8.0000e-05\nEpoch 21/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1765 - lr: 8.0000e-05\nEpoch 22/200\n106/105 [==============================] - 12s 114ms/step - loss: 0.1755 - lr: 8.0000e-05\nEpoch 23/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1741 - lr: 8.0000e-05\nEpoch 24/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1723 - lr: 8.0000e-05\nEpoch 25/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1706 - lr: 8.0000e-05\nEpoch 26/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1690 - lr: 8.0000e-05\nEpoch 27/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1679 - lr: 8.0000e-05\nEpoch 28/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1664 - lr: 8.0000e-05\nEpoch 29/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1649 - lr: 8.0000e-05\nEpoch 30/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1638 - lr: 8.0000e-05\nEpoch 31/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1625 - lr: 8.0000e-05\nEpoch 32/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1614 - lr: 8.0000e-05\nEpoch 33/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1599 - lr: 8.0000e-05\nEpoch 34/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1586 - lr: 8.0000e-05\nEpoch 35/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1579 - lr: 8.0000e-05\nEpoch 36/200\n106/105 [==============================] - 12s 114ms/step - loss: 0.1564 - lr: 8.0000e-05\nEpoch 37/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1556 - lr: 8.0000e-05\nEpoch 38/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1535 - lr: 8.0000e-05\nEpoch 39/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1518 - lr: 8.0000e-05\nEpoch 40/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1506 - lr: 8.0000e-05\nEpoch 41/200\n106/105 [==============================] - 12s 116ms/step - loss: 0.1494 - lr: 8.0000e-05\nEpoch 42/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1480 - lr: 8.0000e-05\nEpoch 43/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1471 - lr: 8.0000e-05\nEpoch 44/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1460 - lr: 8.0000e-05\nEpoch 45/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1447 - lr: 8.0000e-05\nEpoch 46/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1433 - lr: 8.0000e-05\nEpoch 47/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1420 - lr: 8.0000e-05\nEpoch 48/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1406 - lr: 8.0000e-05\nEpoch 49/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1396 - lr: 8.0000e-05\nEpoch 50/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1383 - lr: 8.0000e-05\nEpoch 51/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1371 - lr: 8.0000e-05\nEpoch 52/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1360 - lr: 8.0000e-05\nEpoch 53/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1349 - lr: 8.0000e-05\nEpoch 54/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1338 - lr: 8.0000e-05\nEpoch 55/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1327 - lr: 8.0000e-05\nEpoch 56/200\n106/105 [==============================] - 12s 116ms/step - loss: 0.1317 - lr: 8.0000e-05\nEpoch 57/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1306 - lr: 8.0000e-05\nEpoch 58/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1296 - lr: 8.0000e-05\nEpoch 59/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1283 - lr: 8.0000e-05\nEpoch 60/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1273 - lr: 8.0000e-05\nEpoch 61/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1269 - lr: 8.0000e-05\nEpoch 62/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1260 - lr: 8.0000e-05\nEpoch 63/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1246 - lr: 8.0000e-05\nEpoch 64/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1235 - lr: 8.0000e-05\nEpoch 65/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1221 - lr: 8.0000e-05\nEpoch 66/200\n106/105 [==============================] - 12s 116ms/step - loss: 0.1208 - lr: 8.0000e-05\nEpoch 67/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1197 - lr: 8.0000e-05\nEpoch 68/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1186 - lr: 8.0000e-05\nEpoch 69/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1176 - lr: 8.0000e-05\nEpoch 70/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1168 - lr: 8.0000e-05\nEpoch 71/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1159 - lr: 8.0000e-05\nEpoch 72/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1151 - lr: 8.0000e-05\nEpoch 73/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1142 - lr: 8.0000e-05\nEpoch 74/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1134 - lr: 8.0000e-05\nEpoch 75/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1126 - lr: 8.0000e-05\nEpoch 76/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.1115 - lr: 8.0000e-05\nEpoch 77/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1103 - lr: 8.0000e-05\nEpoch 78/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1093 - lr: 8.0000e-05\nEpoch 79/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1083 - lr: 8.0000e-05\nEpoch 80/200\n","name":"stdout"},{"output_type":"stream","text":"106/105 [==============================] - 12s 113ms/step - loss: 0.1073 - lr: 8.0000e-05\nEpoch 81/200\n106/105 [==============================] - 12s 114ms/step - loss: 0.1063 - lr: 8.0000e-05\nEpoch 82/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1055 - lr: 8.0000e-05\nEpoch 83/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1046 - lr: 8.0000e-05\nEpoch 84/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.1038 - lr: 8.0000e-05\nEpoch 85/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.1033 - lr: 8.0000e-05\nEpoch 86/200\n106/105 [==============================] - 12s 117ms/step - loss: 0.1030 - lr: 8.0000e-05\nEpoch 87/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.1027 - lr: 8.0000e-05\nEpoch 88/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.1012 - lr: 8.0000e-05\nEpoch 89/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.0998 - lr: 8.0000e-05\nEpoch 90/200\n106/105 [==============================] - 12s 113ms/step - loss: 0.0987 - lr: 8.0000e-05\nEpoch 91/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.0978 - lr: 8.0000e-05\nEpoch 92/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.0969 - lr: 8.0000e-05\nEpoch 93/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.0961 - lr: 8.0000e-05\nEpoch 94/200\n106/105 [==============================] - 12s 110ms/step - loss: 0.0953 - lr: 8.0000e-05\nEpoch 95/200\n106/105 [==============================] - 12s 111ms/step - loss: 0.0954 - lr: 8.0000e-05\nEpoch 96/200\n106/105 [==============================] - 12s 115ms/step - loss: 0.0950 - lr: 8.0000e-05\nEpoch 97/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.0945 - lr: 8.0000e-05\nEpoch 98/200\n106/105 [==============================] - 12s 112ms/step - loss: 0.0933 - lr: 8.0000e-05\nEpoch 99/200\n 19/105 [====>.........................] - ETA: 9s - loss: 0.0806","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-4ed10e28428b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\n    'my_file', overwrite=True, include_optimizer=True, save_format='h5',\n    signatures=None, options=None\n)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('wordtoix.npy', wordtoix) \nnp.save('ixtoword.npy', ixtoword) ","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm -rf /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = tf.keras.models.load_model('../input/language-model/my_file')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummy = ['two','dogs' , 'fighting']\nX_final = X_dummy","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlength = 30\nfor alpha in (range(length)):\n  \n  X_gamma = word_to_ix(X_dummy , wordtoix)\n  X_alpha = np.expand_dims(np.array(X_gamma) , axis = 0)\n  yhat = model.predict(X_alpha)\n  yhat = np.argmax(yhat)\n  word_to_add = ix_to_word([yhat] , ixtoword)\n  X_final.append(word_to_add[0])\n  print(X_dummy)\n  #print(word_to_add[0])\n  X_dummy.append(word_to_add[0])\n  if(len(X_dummy)>12):\n        X_dummy.pop(0)\n\n#print(X_dummy)\n","execution_count":40,"outputs":[{"output_type":"stream","text":"['two', 'dogs', 'fighting', 'between']\n['two', 'dogs', 'fighting', 'between', 'between', 'the']\n['two', 'dogs', 'fighting', 'between', 'between', 'the', 'the', 'coachman']\n['two', 'dogs', 'fighting', 'between', 'between', 'the', 'the', 'coachman', 'coachman', 'the']\n['two', 'dogs', 'fighting', 'between', 'between', 'the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman']\n['dogs', 'fighting', 'between', 'between', 'the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after']\n['fighting', 'between', 'between', 'the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and']\n['between', 'between', 'the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those']\n['between', 'the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had']\n['the', 'the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but']\n['the', 'coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it']\n['coachman', 'coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my']\n['coachman', 'the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face']\n['the', 'the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but']\n['the', 'coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there']\n['coachman', 'coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were']\n['coachman', 'after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not']\n['after', 'after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my']\n['after', 'and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband']\n['and', 'and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in']\n['and', 'those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my']\n['those', 'those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart']\n['those', 'had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed']\n['had', 'had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to']\n['had', 'but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself']\n['but', 'but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself', 'myself', 'to']\n['but', 'it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself', 'myself', 'to', 'to', 'be']\n['it', 'it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself', 'myself', 'to', 'to', 'be', 'be', 'and']\n['it', 'my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself', 'myself', 'to', 'to', 'be', 'be', 'and', 'and', 'whom']\n['my', 'my', 'face', 'face', 'but', 'but', 'there', 'there', 'were', 'were', 'not', 'not', 'my', 'my', 'husband', 'husband', 'in', 'in', 'my', 'my', 'heart', 'heart', 'seemed', 'seemed', 'to', 'to', 'myself', 'myself', 'to', 'to', 'be', 'be', 'and', 'and', 'whom', 'whom', 'he']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('wordtoix.npy', wordtoix) \n\n# Load\n#ixtoword_copy = np.load('ixtoword.npy',allow_pickle='TRUE').item()\n#print(ixtoword_copy['boy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_file = open(\"ixtoword.json\", \"r\")\nixtoword_copy = a_file.read()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ixtoword_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_file = model.to_json()\nwith open(json_file_path, \"w\") as file:\n   file.write(json_file)\n# serialize weights to HDF5\nmodel.save_weights(h5_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm -rf /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}