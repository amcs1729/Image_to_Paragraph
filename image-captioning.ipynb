{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tqdm import tqdm\nimport string \n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/flickr8k/captions.txt' , sep=',')\ndf = df.rename(columns={\"image\": \"image_name\", \"caption\": \"image_caption\"})\ndf['image_caption'] = df['image_caption'].astype(str)\ndf['image_name'] = df['image_name'].astype(str)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_caption_list(df):\n    caption_list = df['image_caption'].to_list()\n\n    def clean_captions(caption_list):\n        table = str.maketrans('', '', string.punctuation)\n\n        for i in tqdm(range(len(caption_list))):\n                desc = caption_list[i]\n                # tokenize\n                desc = desc.split()\n                # convert to lower case\n                desc = [word.lower() for word in desc]\n                # remove punctuation from each token\n                desc = [w.translate(table) for w in desc]\n                # remove hanging 's' and 'a'\n                desc = [word for word in desc if len(word)>1]\n                # remove tokens with numbers in them\n                desc = [word for word in desc if word.isalpha()]\n                # store as string\n                caption_list[i] =  ' '.join(desc)\n\n        return (caption_list)\n    caption_list = clean_captions(caption_list)\n    \n    for i, caption in enumerate(caption_list):\n        caption_list[i] = '<start> ' + caption + ' <end>'\n    \n    \n    return caption_list","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"caption_list = get_caption_list(df)\ndf['image_caption'] = caption_list","execution_count":4,"outputs":[{"output_type":"stream","text":"100%|██████████| 40455/40455 [00:00<00:00, 73807.60it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = []\nfor i, key  in enumerate(caption_list):\n    word_list = (caption_list[i].split())\n    for word in word_list:\n        vocabulary.append(word)\nprint('Original Vocabulary Size: %d' % len(vocabulary))","execution_count":5,"outputs":[{"output_type":"stream","text":"Original Vocabulary Size: 453811\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom collections import Counter \n  \ndef removeElements(lst, k): \n    counted = Counter(lst) \n    return [el for el in lst if counted[el] >= k] \n\n#k = 8\n#vocabulary = ((removeElements(vocabulary, k))) \n\nvocabulary = set(vocabulary)\n#vocabulary.update(['<unk>'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocabulary:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ixtoword = np.load('../input/fork-of-image-captioning/ixtoword.npy',allow_pickle='TRUE').item()\nwordtoix = np.load('../input/fork-of-image-captioning/wordtoix.npy',allow_pickle='TRUE').item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(len(d.split()) for d in caption_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfor i in range(75):\n    \n    my_len = len(list((d) for d in caption_list if((len(d.split()))>= i)))\n    \n    if (my_len != 0):\n        \n        print(\"LEN {}.....{}\".format(i, ( my_len)))\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"caption_df =  pd.Series(caption_list, name ='image_caption') \nimage_name_df = df['image_name']\ndf_new = pd.concat([image_name_df, caption_df], axis=1)\ndf_new = df_new.dropna(axis= 0)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                      image_name  \\\n0      1000268201_693b08cb0e.jpg   \n1      1000268201_693b08cb0e.jpg   \n2      1000268201_693b08cb0e.jpg   \n3      1000268201_693b08cb0e.jpg   \n4      1000268201_693b08cb0e.jpg   \n...                          ...   \n40450   997722733_0cb5439472.jpg   \n40451   997722733_0cb5439472.jpg   \n40452   997722733_0cb5439472.jpg   \n40453   997722733_0cb5439472.jpg   \n40454   997722733_0cb5439472.jpg   \n\n                                           image_caption  \n0      <start> child in pink dress is climbing up set...  \n1          <start> girl going into wooden building <end>  \n2      <start> little girl climbing into wooden playh...  \n3      <start> little girl climbing the stairs to her...  \n4      <start> little girl in pink dress going into w...  \n...                                                  ...  \n40450   <start> man in pink shirt climbs rock face <end>  \n40451  <start> man is rock climbing high in the air <...  \n40452  <start> person in red shirt climbing up rock f...  \n40453            <start> rock climber in red shirt <end>  \n40454  <start> rock climber practices on rock climbin...  \n\n[40455 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>image_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>&lt;start&gt; child in pink dress is climbing up set...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>&lt;start&gt; girl going into wooden building &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>&lt;start&gt; little girl climbing into wooden playh...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>&lt;start&gt; little girl climbing the stairs to her...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>&lt;start&gt; little girl in pink dress going into w...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40450</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>&lt;start&gt; man in pink shirt climbs rock face &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>40451</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>&lt;start&gt; man is rock climbing high in the air &lt;...</td>\n    </tr>\n    <tr>\n      <th>40452</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>&lt;start&gt; person in red shirt climbing up rock f...</td>\n    </tr>\n    <tr>\n      <th>40453</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>&lt;start&gt; rock climber in red shirt &lt;end&gt;</td>\n    </tr>\n    <tr>\n      <th>40454</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>&lt;start&gt; rock climber practices on rock climbin...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40455 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train , df_test = train_test_split(df,test_size=0.33, random_state=42)\ndf_train , df_val = train_test_split(df_train, test_size = 0.2 , random_state= 42)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def same_length_caption(caption , max_len=50):\n\n    '''\n    Takes caption as input and makes them of equal length\n    \n    Parameters:-\n    caption (list) - The list of embedded caption to be made of particular length\n    max_len (int) - The max length of the caption\n    \n    Return type:-\n    \n    caption (list) :- Returns a list with zero padding of length = max_len\n    '''\n    \n    \n    if(len(caption) == max_len):\n        return (caption)\n    else:\n        for i in range((max_len-len(caption))):\n            caption.append(0)\n    return caption","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_to_ix(caption , vocab):\n    '''\n    Maps the words to integers according to custom vocabulary\n    \n    Parameters:-\n    caption (list) - The caption to be embedded\n    vocab (dict) - The custom mapping that wil be used as vocabulary\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after mapping them according to 'vocab'\n    '''\n        \n    transformed_caption=[]\n    for word in caption:\n        if (word in wordtoix.keys()):\n            transformed_caption.append(wordtoix[word])\n        #else:\n        #    transformed_caption.append(wordtoix['<unk>'])\n    return (transformed_caption)\n        ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ix_to_word(caption , vocab):\n    '''\n    Takes caption as input and maps them to words as defined by 'vocab'\n    \n    Parameters:-\n    caption (list) - The list of embedded caption to be made of particular length\n    vocab (dict) - The dictionary that wil be used as mapping\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after converting respective integers to words according to vocab\n    '''\n    \n    transformed_caption=[]\n    for word in caption:\n        if (word in ixtoword.keys()):\n            transformed_caption.append(ixtoword[word])\n        else:\n            transformed_caption.append('<unk>')\n    return (transformed_caption)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(samples, batch_size=32):\n    \"\"\"\n    Yields the next training batch.\n    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].\n    \"\"\"\n    num_samples = len(samples)\n    \n    while True:\n        for offset in range(0, num_samples, batch_size):\n            batch_samples = samples.iloc[offset:offset+batch_size]\n\n            input_seq=[]\n            output_seq=[]\n            features_list = []\n\n            for batch_sample in batch_samples.index:\n\n\n                caption_text = batch_samples.at[batch_sample , 'image_caption']\n                caption = caption_text.split()\n                caption = word_to_ix(caption , wordtoix)\n                caption = same_length_caption(caption , max_len = 34)\n\n\n                image_name = batch_samples.at[batch_sample , 'image_name']\n                features = np.load('../input/image-caption-dataset/' \n                                   + image_name[0:-4]\n                                   +'.npy'\n                                  )\n                features = (features).tolist()\n\n\n                features_list.append(features)\n                input_seq.append(caption[:-1])\n                output_seq.append(caption[1:])\n\n\n\n\n            features_list = np.array(features_list)\n            input_seq = np.array(input_seq)\n            output_seq = np.array(output_seq)\n\n\n            #if((features_list.shape != (32,2048)) or (input_seq.shape != (32,33)  )   or (output_seq.shape != (32,33) )):\n            #    print(\"_________________________________________________________________________________\")\n            #    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n            #    print(batch_samples.index)\n            #    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n            #    print(\"FEATURES_LIST SHAPE  -  {}\".format(features_list.shape))\n            #    print(\"INPUT_SEQ SHAPE  -  {}\".format(input_seq.shape))\n            #    print(\"OUTPUT_SEQ SHAPE  -  {}\".format(output_seq.shape))\n            #    print(\"_________________________________________________________________________________\")\n\n\n            yield [features_list, input_seq] ,output_seq\n            #input_seq","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_generator = generator(df_train,batch_size=1)\nX_dummy , y_dummy = next(train_generator)\nprint(X_dummy[0].shape)\nprint(X_dummy[1].shape)\nprint(y_dummy.shape)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nglove_dir = '../input/glove/glove.6B.200d.txt'\nembeddings_index = {} # empty dictionary\nf = open(glove_dir, encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(wordtoix) + 1","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 200\n# Get 200-dim dense vector for each of the 10000 words in out vocabulary\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    #if i < max_words:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in the embedding index will be all zeros\n        embedding_matrix[i] = embedding_vector","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 34","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs1 = tf.keras.layers.Input(shape=(64 , 2048))\nfe2 = tf.keras.layers.GRU(200, return_sequences = False)(inputs1)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fe2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs2 = tf.keras.layers.Input(shape=(max_length-1))\nse1 = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim)(inputs2)\nse2 = tf.keras.layers.GRU(200,return_sequences=True )(se1 , initial_state =fe2)\nse3 = tf.keras.layers.GRU(200,return_sequences=True )(se2 , initial_state =fe2)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"att = tf.keras.layers.Attention(512)([se2 , fe2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = tf.keras.layers.Dense(vocab_size,activation='softmax')(se3)\nmodel = tf.keras.Model(inputs=[inputs1, inputs2],\n                      outputs=[output])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":22,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 33)]         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 64, 2048)]   0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 33, 200)      1753200     input_2[0][0]                    \n__________________________________________________________________________________________________\ngru (GRU)                       (None, 200)          1350000     input_1[0][0]                    \n__________________________________________________________________________________________________\ngru_1 (GRU)                     (None, 33, 200)      241200      embedding[0][0]                  \n                                                                 gru[0][0]                        \n__________________________________________________________________________________________________\ngru_2 (GRU)                     (None, 33, 200)      241200      gru_1[0][0]                      \n                                                                 gru[0][0]                        \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 33, 8766)     1761966     gru_2[0][0]                      \n==================================================================================================\nTotal params: 5,347,566\nTrainable params: 5,347,566\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[2]","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f44bc165e90>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nopt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n\nmodel.compile(loss= 'sparse_categorical_crossentropy', optimizer=opt , metrics= ['accuracy'])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_red = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_accuracy', factor=0.2, patience=2, verbose=0, mode='auto',\n    min_delta=0.0001, cooldown=0, min_lr=0.0000001\n)\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',baseline=None, restore_best_weights=True)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = generator(df_train,batch_size=BATCH_SIZE)\nval_generator = generator(df_val,batch_size=BATCH_SIZE)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n        train_generator,\n        validation_data=val_generator,\n        validation_steps=(len(df_val)/BATCH_SIZE),\n        steps_per_epoch= (len(df_train)/BATCH_SIZE),\n        epochs=200, verbose=1,\n        callbacks = [lr_red, es]\na","execution_count":30,"outputs":[{"output_type":"stream","text":"Epoch 1/200\n85/84 [==============================] - 727s 9s/step - loss: 1.8788 - accuracy: 0.7208 - val_loss: 1.3855 - val_accuracy: 0.7583 - lr: 0.0100\nEpoch 2/200\n85/84 [==============================] - 693s 8s/step - loss: 1.2634 - accuracy: 0.7695 - val_loss: 1.2113 - val_accuracy: 0.7751 - lr: 0.0100\nEpoch 3/200\n85/84 [==============================] - 690s 8s/step - loss: 1.1059 - accuracy: 0.7825 - val_loss: 1.1700 - val_accuracy: 0.7782 - lr: 0.0100\nEpoch 4/200\n85/84 [==============================] - 692s 8s/step - loss: 0.9990 - accuracy: 0.7919 - val_loss: 1.1367 - val_accuracy: 0.7836 - lr: 0.0100\nEpoch 5/200\n85/84 [==============================] - 692s 8s/step - loss: 0.9111 - accuracy: 0.8009 - val_loss: 1.1489 - val_accuracy: 0.7840 - lr: 0.0100\nEpoch 6/200\n85/84 [==============================] - 688s 8s/step - loss: 0.8344 - accuracy: 0.8100 - val_loss: 1.1539 - val_accuracy: 0.7846 - lr: 0.0100\nEpoch 7/200\n85/84 [==============================] - 686s 8s/step - loss: 0.7678 - accuracy: 0.8190 - val_loss: 1.1635 - val_accuracy: 0.7866 - lr: 0.0100\nEpoch 8/200\n85/84 [==============================] - 686s 8s/step - loss: 0.7137 - accuracy: 0.8276 - val_loss: 1.1825 - val_accuracy: 0.7876 - lr: 0.0100\nEpoch 9/200\n85/84 [==============================] - 688s 8s/step - loss: 0.6657 - accuracy: 0.8358 - val_loss: 1.2222 - val_accuracy: 0.7863 - lr: 0.0100\nEpoch 10/200\n85/84 [==============================] - 691s 8s/step - loss: 0.6281 - accuracy: 0.8422 - val_loss: 1.2260 - val_accuracy: 0.7840 - lr: 0.0100\nEpoch 11/200\n85/84 [==============================] - 694s 8s/step - loss: 0.5235 - accuracy: 0.8653 - val_loss: 1.1912 - val_accuracy: 0.7906 - lr: 0.0020\nEpoch 12/200\n85/84 [==============================] - 692s 8s/step - loss: 0.4713 - accuracy: 0.8788 - val_loss: 1.1985 - val_accuracy: 0.7904 - lr: 0.0020\nEpoch 13/200\n85/84 [==============================] - 701s 8s/step - loss: 0.4391 - accuracy: 0.8874 - val_loss: 1.2091 - val_accuracy: 0.7902 - lr: 0.0020\nEpoch 14/200\n85/84 [==============================] - 701s 8s/step - loss: 0.4271 - accuracy: 0.8892 - val_loss: 1.2019 - val_accuracy: 0.7912 - lr: 4.0000e-04\nEpoch 15/200\n85/84 [==============================] - 704s 8s/step - loss: 0.4139 - accuracy: 0.8933 - val_loss: 1.2032 - val_accuracy: 0.7913 - lr: 4.0000e-04\nEpoch 16/200\n85/84 [==============================] - 704s 8s/step - loss: 0.4060 - accuracy: 0.8955 - val_loss: 1.2050 - val_accuracy: 0.7912 - lr: 4.0000e-04\nEpoch 17/200\n85/84 [==============================] - 718s 8s/step - loss: 0.4039 - accuracy: 0.8956 - val_loss: 1.2026 - val_accuracy: 0.7913 - lr: 8.0000e-05\nEpoch 18/200\n85/84 [==============================] - 727s 9s/step - loss: 0.4000 - accuracy: 0.8968 - val_loss: 1.2023 - val_accuracy: 0.7913 - lr: 8.0000e-05\nEpoch 19/200\n85/84 [==============================] - 734s 9s/step - loss: 0.3977 - accuracy: 0.8975 - val_loss: 1.2021 - val_accuracy: 0.7913 - lr: 1.6000e-05\nEpoch 20/200\n85/84 [==============================] - 739s 9s/step - loss: 0.3971 - accuracy: 0.8977 - val_loss: 1.2021 - val_accuracy: 0.7913 - lr: 1.6000e-05\nEpoch 21/200\n85/84 [==============================] - 732s 9s/step - loss: 0.3966 - accuracy: 0.8978 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.6000e-05\nEpoch 22/200\n85/84 [==============================] - 736s 9s/step - loss: 0.3962 - accuracy: 0.8979 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.6000e-05\nEpoch 23/200\n85/84 [==============================] - 767s 9s/step - loss: 0.3956 - accuracy: 0.8981 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 3.2000e-06\nEpoch 24/200\n85/84 [==============================] - 751s 9s/step - loss: 0.3955 - accuracy: 0.8981 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 3.2000e-06\nEpoch 25/200\n85/84 [==============================] - 757s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 6.4000e-07\nEpoch 26/200\n85/84 [==============================] - 805s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 6.4000e-07\nEpoch 27/200\n85/84 [==============================] - 772s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.2800e-07\nEpoch 28/200\n 2/84 [..............................] - ETA: 4:43 - loss: 0.4336 - accuracy: 0.8883","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-159c6b8e8811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr_red\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.backend import manual_variable_initialization\nmanual_variable_initialization(True)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\n    'my_file', overwrite=True, include_optimizer=True, save_format='h5',\n    signatures=None, options=None\n)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('wordtoix.npy', wordtoix) \nnp.save('ixtoword.npy', ixtoword)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel model\ndel inputs1\ndel se1\ndel se2\ndel output\ndel input\ndel fe2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.load_model('../input/fork-of-image-captioning/my_file')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_copy =  df_test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"display(Image(filename='../input/flickr8k/Images/'+df_test_copy.at[2, 'image_name']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator = generator(df_test_copy , 100)\nX_dummy , y_dummy = next(test_generator)\nyhat = model.predict(X_dummy)\ncaption_list = [] \nfor i in (range(len(yhat))):\n              #display(Image(filename='../input/flickr8k/Images/'+df_test_copy.at[i, 'image_name']))\n              \n              predicted = yhat[i,:,:]\n              \n              s= []\n              \n              for each in range(33):\n                  s.append(ix_to_word([np.argmax(predicted[each])] , ixtoword)[0])\n                  \n                  \n                            \n              #caption = \"\"\n              #caption = (caption.join(s))\n            \n              cleaned_s = []\n              \n              for each in range(33):\n                   \n                    if (not(s[each]=='<unk>' or s[each]=='<end>')):\n                        \n                        cleaned_s.append(s[each])\n              caption_list.append(cleaned_s)\n              \n              \n                          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in range(len(caption_list)):\n    display(Image(filename='../input/flickr8k/Images/'+df_test_copy.at[each, 'image_name']))\n    \n    print(caption_list[each])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_to_ix(caption , vocab):\n    '''\n    Maps the words to integers according to custom vocabulary\n    \n    Parameters:-\n    caption (list) - The caption to be embedded\n    vocab (dict) - The custom mapping that wil be used as vocabulary\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after mapping them according to 'vocab'\n    '''\n        \n    transformed_caption=[]\n    for word in caption:\n        if (word in vocab.keys()):\n            transformed_caption.append(vocab[word])\n        else:\n            transformed_caption.append(vocab['<unk>'])\n    return (transformed_caption)\n        \ndef ix_to_word(caption , vocab):\n    '''\n    Takes caption as input and maps them to words as defined by 'vocab'\n    \n    Parameters:-\n    caption (list) - The list of embedded caption to be made of particular length\n    vocab (dict) - The dictionary that wil be used as mapping\n    \n    Return type:-\n    \n    caption (list) :- Returns a list after converting respective integers to words according to vocab\n    '''\n    \n    transformed_caption=[]\n    for word in caption:\n        transformed_caption.append(vocab[word])\n\n    return (transformed_caption)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ixtoword_lang = np.load('../input/language-model/ixtoword.npy',allow_pickle='TRUE').item()\nwordtoix_lang = np.load('../input/language-model/wordtoix.npy',allow_pickle='TRUE').item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"language_model = model = tf.keras.models.load_model('../input/language-model/my_file')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummy = ['two' ,'dogs','fighting','in']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final = ['two' ,'dogs','fighting','in']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = 30\n\nfor i in (range(length)):\n    if (len(X_dummy)>12):\n        X_dummy.pop(0)\n    \n    X_gamma = word_to_ix(X_dummy , wordtoix_lang)\n    X_alpha = np.expand_dims(np.array(X_gamma) , axis = 0)\n    \n    yhat = model.predict(X_alpha)\n    yhat = np.argmax(yhat)\n    word_to_add = ix_to_word([yhat] , ixtoword_lang)\n    X_dummy.append(word_to_add[0])\n    X_final.append(word_to_add[0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}