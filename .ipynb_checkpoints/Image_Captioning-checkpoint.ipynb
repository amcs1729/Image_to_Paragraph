{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/flickr8k/captions.txt' , sep=',')\n",
    "df = df.rename(columns={\"image\": \"image_name\", \"caption\": \"image_caption\"})\n",
    "df['image_caption'] = df['image_caption'].astype(str)\n",
    "df['image_name'] = df['image_name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_list(df):\n",
    "    caption_list = df['image_caption'].to_list()\n",
    "\n",
    "    def clean_captions(caption_list):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "        for i in tqdm(range(len(caption_list))):\n",
    "                desc = caption_list[i]\n",
    "                # tokenize\n",
    "                desc = desc.split()\n",
    "                # convert to lower case\n",
    "                desc = [word.lower() for word in desc]\n",
    "                # remove punctuation from each token\n",
    "                desc = [w.translate(table) for w in desc]\n",
    "                # remove hanging 's' and 'a'\n",
    "                desc = [word for word in desc if len(word)>1]\n",
    "                # remove tokens with numbers in them\n",
    "                desc = [word for word in desc if word.isalpha()]\n",
    "                # store as string\n",
    "                caption_list[i] =  ' '.join(desc)\n",
    "\n",
    "        return (caption_list)\n",
    "    caption_list = clean_captions(caption_list)\n",
    "    \n",
    "    for i, caption in enumerate(caption_list):\n",
    "        caption_list[i] = '<start> ' + caption + ' <end>'\n",
    "    \n",
    "    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40455/40455 [00:00<00:00, 73807.60it/s]\n"
     ]
    }
   ],
   "source": [
    "caption_list = get_caption_list(df)\n",
    "df['image_caption'] = caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 453811\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for i, key  in enumerate(caption_list):\n",
    "    word_list = (caption_list[i].split())\n",
    "    for word in word_list:\n",
    "        vocabulary.append(word)\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter \n",
    "#def removeElements(lst, k): \n",
    "#    counted = Counter(lst) \n",
    "#    return [el for el in lst if counted[el] >= k] \n",
    "#k = 8\n",
    "#vocabulary = ((removeElements(vocabulary, k))) \n",
    "#vocabulary.update(['<unk>'])\n",
    "vocabulary = set(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocabulary:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword = np.load('../input/fork-of-image-captioning/ixtoword.npy',allow_pickle='TRUE').item()\n",
    "wordtoix = np.load('../input/fork-of-image-captioning/wordtoix.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(d.split()) for d in caption_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_df =  pd.Series(caption_list, name ='image_caption') \n",
    "image_name_df = df['image_name']\n",
    "df_new = pd.concat([image_name_df, caption_df], axis=1)\n",
    "df_new = df_new.dropna(axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>&lt;start&gt; child in pink dress is climbing up set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>&lt;start&gt; girl going into wooden building &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>&lt;start&gt; little girl climbing into wooden playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>&lt;start&gt; little girl climbing the stairs to her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>&lt;start&gt; little girl in pink dress going into w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>&lt;start&gt; man in pink shirt climbs rock face &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>&lt;start&gt; man is rock climbing high in the air &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>&lt;start&gt; person in red shirt climbing up rock f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>&lt;start&gt; rock climber in red shirt &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>&lt;start&gt; rock climber practices on rock climbin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image_name  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                           image_caption  \n",
       "0      <start> child in pink dress is climbing up set...  \n",
       "1          <start> girl going into wooden building <end>  \n",
       "2      <start> little girl climbing into wooden playh...  \n",
       "3      <start> little girl climbing the stairs to her...  \n",
       "4      <start> little girl in pink dress going into w...  \n",
       "...                                                  ...  \n",
       "40450   <start> man in pink shirt climbs rock face <end>  \n",
       "40451  <start> man is rock climbing high in the air <...  \n",
       "40452  <start> person in red shirt climbing up rock f...  \n",
       "40453            <start> rock climber in red shirt <end>  \n",
       "40454  <start> rock climber practices on rock climbin...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train , df_test = train_test_split(df,test_size=0.33, random_state=42)\n",
    "df_train , df_val = train_test_split(df_train, test_size = 0.2 , random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_length_caption(caption , max_len=50):\n",
    "\n",
    "    '''\n",
    "    Takes caption as input and makes them of equal length\n",
    "    \n",
    "    Parameters:-\n",
    "    caption (list) - The list of embedded caption to be made of particular length\n",
    "    max_len (int) - The max length of the caption\n",
    "    \n",
    "    Return type:-\n",
    "    \n",
    "    caption (list) :- Returns a list with zero padding of length = max_len\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if(len(caption) == max_len):\n",
    "        return (caption)\n",
    "    else:\n",
    "        for i in range((max_len-len(caption))):\n",
    "            caption.append(0)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_ix(caption , vocab):\n",
    "    '''\n",
    "    Maps the words to integers according to custom vocabulary\n",
    "    \n",
    "    Parameters:-\n",
    "    caption (list) - The caption to be embedded\n",
    "    vocab (dict) - The custom mapping that wil be used as vocabulary\n",
    "    \n",
    "    Return type:-\n",
    "    \n",
    "    caption (list) :- Returns a list after mapping them according to 'vocab'\n",
    "    '''\n",
    "        \n",
    "    transformed_caption=[]\n",
    "    for word in caption:\n",
    "        if (word in wordtoix.keys()):\n",
    "            transformed_caption.append(wordtoix[word])\n",
    "        #else:\n",
    "        #    transformed_caption.append(wordtoix['<unk>'])\n",
    "    return (transformed_caption)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ix_to_word(caption , vocab):\n",
    "    '''\n",
    "    Takes caption as input and maps them to words as defined by 'vocab'\n",
    "    \n",
    "    Parameters:-\n",
    "    caption (list) - The list of embedded caption to be made of particular length\n",
    "    vocab (dict) - The dictionary that wil be used as mapping\n",
    "    \n",
    "    Return type:-\n",
    "    \n",
    "    caption (list) :- Returns a list after converting respective integers to words according to vocab\n",
    "    '''\n",
    "    \n",
    "    transformed_caption=[]\n",
    "    for word in caption:\n",
    "        if (word in ixtoword.keys()):\n",
    "            transformed_caption.append(ixtoword[word])\n",
    "        else:\n",
    "            transformed_caption.append('<unk>')\n",
    "    return (transformed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "\n",
    "            input_seq=[]\n",
    "            output_seq=[]\n",
    "            features_list = []\n",
    "\n",
    "            for batch_sample in batch_samples.index:\n",
    "                caption_text = batch_samples.at[batch_sample , 'image_caption']\n",
    "                caption = caption_text.split()\n",
    "                caption = word_to_ix(caption , wordtoix)\n",
    "                caption = same_length_caption(caption , max_len = 34)\n",
    "                image_name = batch_samples.at[batch_sample , 'image_name']\n",
    "                features = np.load('../input/image-caption-dataset/' \n",
    "                                   + image_name[0:-4]\n",
    "                                   +'.npy'\n",
    "                                  )\n",
    "                features = (features).tolist()\n",
    "                features_list.append(features)\n",
    "                input_seq.append(caption[:-1])\n",
    "                output_seq.append(caption[1:])\n",
    "            \n",
    "            features_list = np.array(features_list)\n",
    "            input_seq = np.array(input_seq)\n",
    "            output_seq = np.array(output_seq)\n",
    "\n",
    "            yield [features_list, input_seq] ,output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_dir = '../input/glove/glove.6B.200d.txt'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(glove_dir, encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wordtoix) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.keras.layers.Input(shape=(64 , 2048))\n",
    "fe2 = tf.keras.layers.GRU(200, return_sequences = False)(inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tf.keras.layers.Input(shape=(max_length-1))\n",
    "se1 = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim)(inputs2)\n",
    "se2 = tf.keras.layers.GRU(200,return_sequences=True )(se1 , initial_state =fe2)\n",
    "se3 = tf.keras.layers.GRU(200,return_sequences=True )(se2 , initial_state =fe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = tf.keras.layers.Attention(512)([se2 , fe2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.Dense(vocab_size,activation='softmax')(se3)\n",
    "model = tf.keras.Model(inputs=[inputs1, inputs2],\n",
    "                      outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 64, 2048)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 200)      1753200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 200)          1350000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 33, 200)      241200      embedding[0][0]                  \n",
      "                                                                 gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 33, 200)      241200      gru_1[0][0]                      \n",
      "                                                                 gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 33, 8766)     1761966     gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,347,566\n",
      "Trainable params: 5,347,566\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f44bc165e90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy', optimizer=opt , metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_red = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.2, patience=2, verbose=0, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0.0000001\n",
    ")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(df_train,batch_size=BATCH_SIZE)\n",
    "val_generator = generator(df_val,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "85/84 [==============================] - 727s 9s/step - loss: 1.8788 - accuracy: 0.7208 - val_loss: 1.3855 - val_accuracy: 0.7583 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "85/84 [==============================] - 693s 8s/step - loss: 1.2634 - accuracy: 0.7695 - val_loss: 1.2113 - val_accuracy: 0.7751 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "85/84 [==============================] - 690s 8s/step - loss: 1.1059 - accuracy: 0.7825 - val_loss: 1.1700 - val_accuracy: 0.7782 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "85/84 [==============================] - 692s 8s/step - loss: 0.9990 - accuracy: 0.7919 - val_loss: 1.1367 - val_accuracy: 0.7836 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "85/84 [==============================] - 692s 8s/step - loss: 0.9111 - accuracy: 0.8009 - val_loss: 1.1489 - val_accuracy: 0.7840 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "85/84 [==============================] - 688s 8s/step - loss: 0.8344 - accuracy: 0.8100 - val_loss: 1.1539 - val_accuracy: 0.7846 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "85/84 [==============================] - 686s 8s/step - loss: 0.7678 - accuracy: 0.8190 - val_loss: 1.1635 - val_accuracy: 0.7866 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "85/84 [==============================] - 686s 8s/step - loss: 0.7137 - accuracy: 0.8276 - val_loss: 1.1825 - val_accuracy: 0.7876 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "85/84 [==============================] - 688s 8s/step - loss: 0.6657 - accuracy: 0.8358 - val_loss: 1.2222 - val_accuracy: 0.7863 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "85/84 [==============================] - 691s 8s/step - loss: 0.6281 - accuracy: 0.8422 - val_loss: 1.2260 - val_accuracy: 0.7840 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "85/84 [==============================] - 694s 8s/step - loss: 0.5235 - accuracy: 0.8653 - val_loss: 1.1912 - val_accuracy: 0.7906 - lr: 0.0020\n",
      "Epoch 12/200\n",
      "85/84 [==============================] - 692s 8s/step - loss: 0.4713 - accuracy: 0.8788 - val_loss: 1.1985 - val_accuracy: 0.7904 - lr: 0.0020\n",
      "Epoch 13/200\n",
      "85/84 [==============================] - 701s 8s/step - loss: 0.4391 - accuracy: 0.8874 - val_loss: 1.2091 - val_accuracy: 0.7902 - lr: 0.0020\n",
      "Epoch 14/200\n",
      "85/84 [==============================] - 701s 8s/step - loss: 0.4271 - accuracy: 0.8892 - val_loss: 1.2019 - val_accuracy: 0.7912 - lr: 4.0000e-04\n",
      "Epoch 15/200\n",
      "85/84 [==============================] - 704s 8s/step - loss: 0.4139 - accuracy: 0.8933 - val_loss: 1.2032 - val_accuracy: 0.7913 - lr: 4.0000e-04\n",
      "Epoch 16/200\n",
      "85/84 [==============================] - 704s 8s/step - loss: 0.4060 - accuracy: 0.8955 - val_loss: 1.2050 - val_accuracy: 0.7912 - lr: 4.0000e-04\n",
      "Epoch 17/200\n",
      "85/84 [==============================] - 718s 8s/step - loss: 0.4039 - accuracy: 0.8956 - val_loss: 1.2026 - val_accuracy: 0.7913 - lr: 8.0000e-05\n",
      "Epoch 18/200\n",
      "85/84 [==============================] - 727s 9s/step - loss: 0.4000 - accuracy: 0.8968 - val_loss: 1.2023 - val_accuracy: 0.7913 - lr: 8.0000e-05\n",
      "Epoch 19/200\n",
      "85/84 [==============================] - 734s 9s/step - loss: 0.3977 - accuracy: 0.8975 - val_loss: 1.2021 - val_accuracy: 0.7913 - lr: 1.6000e-05\n",
      "Epoch 20/200\n",
      "85/84 [==============================] - 739s 9s/step - loss: 0.3971 - accuracy: 0.8977 - val_loss: 1.2021 - val_accuracy: 0.7913 - lr: 1.6000e-05\n",
      "Epoch 21/200\n",
      "85/84 [==============================] - 732s 9s/step - loss: 0.3966 - accuracy: 0.8978 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.6000e-05\n",
      "Epoch 22/200\n",
      "85/84 [==============================] - 736s 9s/step - loss: 0.3962 - accuracy: 0.8979 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.6000e-05\n",
      "Epoch 23/200\n",
      "85/84 [==============================] - 767s 9s/step - loss: 0.3956 - accuracy: 0.8981 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 3.2000e-06\n",
      "Epoch 24/200\n",
      "85/84 [==============================] - 751s 9s/step - loss: 0.3955 - accuracy: 0.8981 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 3.2000e-06\n",
      "Epoch 25/200\n",
      "85/84 [==============================] - 757s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 6.4000e-07\n",
      "Epoch 26/200\n",
      "85/84 [==============================] - 805s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 6.4000e-07\n",
      "Epoch 27/200\n",
      "85/84 [==============================] - 772s 9s/step - loss: 0.3954 - accuracy: 0.8982 - val_loss: 1.2020 - val_accuracy: 0.7913 - lr: 1.2800e-07\n",
      "Epoch 28/200\n",
      " 2/84 [..............................] - ETA: 4:43 - loss: 0.4336 - accuracy: 0.8883"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-159c6b8e8811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr_red\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=(len(df_val)/BATCH_SIZE),\n",
    "        steps_per_epoch= (len(df_train)/BATCH_SIZE),\n",
    "        epochs=200, verbose=1,\n",
    "        callbacks = [lr_red, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import manual_variable_initialization\n",
    "manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\n",
    "    'my_file', overwrite=True, include_optimizer=True, save_format='h5',\n",
    "    signatures=None, options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('wordtoix.npy', wordtoix) \n",
    "np.save('ixtoword.npy', ixtoword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
